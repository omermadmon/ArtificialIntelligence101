{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MDP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UxktCmc2T_d"
      },
      "source": [
        "# MDP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6Bwb6vI2UXL"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from numpy.random import choice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkWoN_ri2eE6"
      },
      "source": [
        "# Problem Definition\n",
        "\n",
        "states = ('PU', 'PF', 'RU', 'RF')\n",
        "actions = ('save_money', 'advertise')\n",
        "\n",
        "def reward(state):\n",
        "    if state.startswith('P'):\n",
        "        return 0\n",
        "    elif state.startswith('R'):\n",
        "        return 10\n",
        "\n",
        "def transition_function(state, action, resulted_state):\n",
        "    transition_dictionary = {\n",
        "        ('PU', 'save_money', 'PU'): 1.0,\n",
        "        ('PU', 'save_money', 'PF'): 0,\n",
        "        ('PU', 'save_money', 'RU'): 0,\n",
        "        ('PU', 'save_money', 'RF'): 0,\n",
        "        ('PU', 'advertise', 'PU'): 0.5,\n",
        "        ('PU', 'advertise', 'PF'): 0.5,\n",
        "        ('PU', 'advertise', 'RU'): 0,\n",
        "        ('PU', 'advertise', 'RF'): 0,\n",
        "\n",
        "        ('PF', 'save_money', 'PU'): 0.5,\n",
        "        ('PF', 'save_money', 'PF'): 0,\n",
        "        ('PF', 'save_money', 'RU'): 0,\n",
        "        ('PF', 'save_money', 'RF'): 0.5,\n",
        "        ('PF', 'advertise', 'PU'): 0,\n",
        "        ('PF', 'advertise', 'PF'): 1.0,\n",
        "        ('PF', 'advertise', 'RU'): 0,\n",
        "        ('PF', 'advertise', 'RF'): 0,\n",
        "\n",
        "        ('RU', 'save_money', 'PU'): 0.5,\n",
        "        ('RU', 'save_money', 'PF'): 0,\n",
        "        ('RU', 'save_money', 'RU'): 0.5,\n",
        "        ('RU', 'save_money', 'RF'): 0,\n",
        "        ('RU', 'advertise', 'PU'): 0.5,\n",
        "        ('RU', 'advertise', 'PF'): 0.5,\n",
        "        ('RU', 'advertise', 'RU'): 0,\n",
        "        ('RU', 'advertise', 'RF'): 0,\n",
        "\n",
        "        ('RF', 'save_money', 'PU'): 0,\n",
        "        ('RF', 'save_money', 'PF'): 0,\n",
        "        ('RF', 'save_money', 'RU'): 0.5,\n",
        "        ('RF', 'save_money', 'RF'): 0.5,\n",
        "        ('RF', 'advertise', 'PU'): 0,\n",
        "        ('RF', 'advertise', 'PF'): 1.0,\n",
        "        ('RF', 'advertise', 'RU'): 0,\n",
        "        ('RF', 'advertise', 'RF'): 0\n",
        "    }\n",
        "    return transition_dictionary[state, action, resulted_state]\n",
        "\n",
        "\n",
        "def get_next_state(state, action):\n",
        "    probabilities = [transition_function(state, action, s) for s in states]\n",
        "    resulted_state = choice(states, p=probabilities)\n",
        "    return resulted_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB44MKHr2d9z"
      },
      "source": [
        "# MDP class implementation\n",
        "\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from random import choice\n",
        "\n",
        "class MDP:\n",
        "    '''\n",
        "    Assuming P and R are known.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, states=states, actions=actions, P=transition_function, R=reward, gamma=0.9):\n",
        "        self.gamma = gamma  # discount constant\n",
        "        self.states = states\n",
        "        self.actions = actions\n",
        "        self.P = P # transition function\n",
        "        self.R = R # reward function\n",
        "        self.V = {} # current est. V*\n",
        "        self.optimal_policy = {}\n",
        "\n",
        "    def choose_action(self, state):\n",
        "      if len(self.optimal_policy) == 0:\n",
        "        return choice(self.actions, 1, p=probs)[0]\n",
        "      else:\n",
        "        return self.optimal_policy[state]\n",
        "\n",
        "    def policy_evaluation(self, policy):\n",
        "      n = len(self.states)\n",
        "      A = np.zeros((n, n))\n",
        "      b = np.array([-self.R(s) for s in self.states])\n",
        "      for i, s in enumerate(self.states):\n",
        "        A[i][i] = -1\n",
        "        for j, s2 in enumerate(self.states):\n",
        "          A[i][j] += self.gamma * self.P(s, policy[s], s2)\n",
        "      values = np.linalg.solve(A, b)\n",
        "      return {s: v for (s, v) in zip(self.states, values)}\n",
        "\n",
        "    def policy_iteration(self, T=100):\n",
        "      # init random policy\n",
        "      old_policy = {s: choice(self.actions) for s in self.states}\n",
        "      for i in range(T):\n",
        "        new_policy = {}\n",
        "        self.V = self.policy_evaluation(old_policy)\n",
        "        for s in self.states:\n",
        "          # argmax_a { sum ( P(s' | s , a) * V(s') ) }\n",
        "          action_index = np.argmax([sum([self.P(s, a, s2) * self.V[s2] for s2 in self.states]) for a in self.actions])\n",
        "          new_policy[s] = self.actions[action_index]\n",
        "        old_policy = new_policy\n",
        "      self.optimal_policy = deepcopy(new_policy)\n",
        "\n",
        "    def value_iteration(self, T=100):\n",
        "      policy = {}\n",
        "      old_V = {s: 0 for s in self.states}\n",
        "      for i in range(T):\n",
        "        new_V = deepcopy(old_V)\n",
        "        for s in self.states:\n",
        "          max_a_exp = float(\"-inf\") # sum ( P(s' | s , a) * V(s') )\n",
        "          max_a = None # a which maximizing max_a_exp\n",
        "          for a in self.actions:\n",
        "            exp = sum([self.P(s, a, s2) * old_V[s2] for s2 in self.states])\n",
        "            if exp > max_a_exp:\n",
        "              max_a_exp = exp\n",
        "              max_a = a\n",
        "          policy[s] = max_a\n",
        "          new_V[s] = self.R(s) + self.gamma * max_a_exp\n",
        "          old_V = deepcopy(new_V)\n",
        "      self.V = deepcopy(new_V)\n",
        "      self.optimal_policy = deepcopy(policy)\n",
        "\n",
        "    def print_policy(self):\n",
        "      if len(self.optimal_policy) == 0:\n",
        "        print('{}')\n",
        "      else:\n",
        "        for s in states:\n",
        "            a = self.optimal_policy[s]\n",
        "            print(f's: {s}, a: {a}, v*(s): {self.V[s]}')\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzNi86zW2d0j",
        "outputId": "eaf51e68-e6b4-46cc-9cba-362fd57f4179"
      },
      "source": [
        "# Usage (VI, PI, policy eval)\n",
        "\n",
        "mdp = MDP()\n",
        "mdp.print_policy()\n",
        "\n",
        "print('value iteration result:')\n",
        "mdp.value_iteration(100)\n",
        "mdp.print_policy()\n",
        "\n",
        "print('policy iteration result:')\n",
        "mdp.policy_iteration(100)\n",
        "mdp.print_policy()\n",
        "\n",
        "phi = {\n",
        "    'PU': 'advertise',\n",
        "    'PF': 'save_money',\n",
        "    'RU': 'save_money',\n",
        "    'RF': 'save_money'\n",
        "}\n",
        "print('policy evaluation on policy phi:')\n",
        "print(mdp.policy_evaluation(phi))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{}\n",
            "value iteration result:\n",
            "s: PU, a: advertise, v*(s): 31.58508953413495\n",
            "s: PF, a: save_money, v*(s): 38.60400287377479\n",
            "s: RU, a: save_money, v*(s): 44.02416232966445\n",
            "s: RF, a: save_money, v*(s): 54.20158563176306\n",
            "\n",
            "policy iteration result:\n",
            "s: PU, a: advertise, v*(s): 31.58510430883212\n",
            "s: PF, a: save_money, v*(s): 38.60401637746148\n",
            "s: RU, a: save_money, v*(s): 44.024176252680824\n",
            "s: RF, a: save_money, v*(s): 54.20159875219339\n",
            "\n",
            "policy evaluation on policy phi:\n",
            "{'PU': 31.58510430883212, 'PF': 38.60401637746148, 'RU': 44.024176252680824, 'RF': 54.20159875219339}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}