{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimistic-RL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UxktCmc2T_d"
      },
      "source": [
        "# Optimistic-RL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6Bwb6vI2UXL"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from numpy.random import choice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkWoN_ri2eE6"
      },
      "source": [
        "# Problem Definition\n",
        "\n",
        "states = ('PU', 'PF', 'RU', 'RF')\n",
        "actions = ('save_money', 'advertise')\n",
        "\n",
        "def reward(state):\n",
        "    if state.startswith('P'):\n",
        "        return 0\n",
        "    elif state.startswith('R'):\n",
        "        return 10\n",
        "\n",
        "def transition_function(state, action, resulted_state):\n",
        "    transition_dictionary = {\n",
        "        ('PU', 'save_money', 'PU'): 1.0,\n",
        "        ('PU', 'save_money', 'PF'): 0,\n",
        "        ('PU', 'save_money', 'RU'): 0,\n",
        "        ('PU', 'save_money', 'RF'): 0,\n",
        "        ('PU', 'advertise', 'PU'): 0.5,\n",
        "        ('PU', 'advertise', 'PF'): 0.5,\n",
        "        ('PU', 'advertise', 'RU'): 0,\n",
        "        ('PU', 'advertise', 'RF'): 0,\n",
        "\n",
        "        ('PF', 'save_money', 'PU'): 0.5,\n",
        "        ('PF', 'save_money', 'PF'): 0,\n",
        "        ('PF', 'save_money', 'RU'): 0,\n",
        "        ('PF', 'save_money', 'RF'): 0.5,\n",
        "        ('PF', 'advertise', 'PU'): 0,\n",
        "        ('PF', 'advertise', 'PF'): 1.0,\n",
        "        ('PF', 'advertise', 'RU'): 0,\n",
        "        ('PF', 'advertise', 'RF'): 0,\n",
        "\n",
        "        ('RU', 'save_money', 'PU'): 0.5,\n",
        "        ('RU', 'save_money', 'PF'): 0,\n",
        "        ('RU', 'save_money', 'RU'): 0.5,\n",
        "        ('RU', 'save_money', 'RF'): 0,\n",
        "        ('RU', 'advertise', 'PU'): 0.5,\n",
        "        ('RU', 'advertise', 'PF'): 0.5,\n",
        "        ('RU', 'advertise', 'RU'): 0,\n",
        "        ('RU', 'advertise', 'RF'): 0,\n",
        "\n",
        "        ('RF', 'save_money', 'PU'): 0,\n",
        "        ('RF', 'save_money', 'PF'): 0,\n",
        "        ('RF', 'save_money', 'RU'): 0.5,\n",
        "        ('RF', 'save_money', 'RF'): 0.5,\n",
        "        ('RF', 'advertise', 'PU'): 0,\n",
        "        ('RF', 'advertise', 'PF'): 1.0,\n",
        "        ('RF', 'advertise', 'RU'): 0,\n",
        "        ('RF', 'advertise', 'RF'): 0\n",
        "    }\n",
        "    return transition_dictionary[state, action, resulted_state]\n",
        "\n",
        "\n",
        "def get_next_state(state, action):\n",
        "    probabilities = [transition_function(state, action, s) for s in states]\n",
        "    resulted_state = choice(states, p=probabilities)\n",
        "    return resulted_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB44MKHr2d9z"
      },
      "source": [
        "# Optimistic-RL class implementation\n",
        "\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "\n",
        "class OptimisticRL:\n",
        "    '''\n",
        "    Assuming P and R are unknown and needed to be estimated.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, states, actions, r_max, gamma=0.9, N_e=3, T=100):\n",
        "        # mapping states and actions to integers:\n",
        "        self.original_states = states\n",
        "        self.original_actions = actions\n",
        "        self.states = list(range(len(states)))\n",
        "        self.actions = list(range(len(actions)))\n",
        "\n",
        "        self.T = T # number of iteration for VI\n",
        "        self.N_e = N_e # explored-enough constant\n",
        "        self.N = defaultdict(int) # number of times action a has been tried in state s\n",
        "        self.P = self.init_probs() # est. current probs\n",
        "        self.V = {s: 0 for s in states} # est. values for all states\n",
        "        self.observed_transitions = 0 * self.init_probs() # number of times action a has been tried in state s and led to state s'\n",
        "        self.gamma = gamma  # discount constant\n",
        "        self.R_max = r_max\n",
        "        self.V_max = self.R_max / (1-gamma)\n",
        "        self.R = {s: self.R_max for s in self.states} # est. current rewards\n",
        "        self.policy = {} # current known policy\n",
        "\n",
        "    def init_probs(self):\n",
        "      n = len(self.states)\n",
        "      m = len(self.actions)\n",
        "      return (1/n) * np.ones((n, m, n))\n",
        "    \n",
        "    def value_iteration(self, optimistic=True):\n",
        "      policy = {}\n",
        "      old_V = {s: 0 for s in self.states}\n",
        "      for i in range(self.T):\n",
        "        new_V = deepcopy(old_V)\n",
        "        for s in self.states:\n",
        "          max_a_exp = float(\"-inf\") # sum ( P(s' | s , a) * V(s') )\n",
        "          max_a = None # a which maximizing max_a_exp\n",
        "          for a in self.actions:\n",
        "            if optimistic and self.N[(s, a)] < self.N_e:\n",
        "              max_a_exp = self.V_max\n",
        "              max_a = a\n",
        "              break\n",
        "            else:\n",
        "              exp = sum([self.P[s][a][s2] * old_V[s2] for s2 in self.states])\n",
        "              if exp > max_a_exp:\n",
        "                max_a_exp = exp\n",
        "                max_a = a\n",
        "          policy[s] = max_a\n",
        "          new_V[s] = self.R[s] + self.gamma * max_a_exp\n",
        "          old_V = deepcopy(new_V)\n",
        "      self.V = deepcopy(new_V)\n",
        "      self.policy = deepcopy(policy)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "      s = self.original_states.index(state)\n",
        "      a = self.policy[s]\n",
        "      action = self.original_actions[a]\n",
        "      return action\n",
        "\n",
        "    def learn(self, s1, a1, reward_state1, s2):\n",
        "      state1 = self.original_states.index(s1)\n",
        "      action1 = self.original_actions.index(a1)\n",
        "      state2 = self.original_states.index(s2)\n",
        "      self.R[state1] = reward_state1\n",
        "      self.N[(state1, action1)] += 1\n",
        "      self.observed_transitions[state1][action1][state2] += 1\n",
        "      self.update_probs()\n",
        "\n",
        "    def update_probs(self):\n",
        "      for state1 in self.states:\n",
        "        for action1 in self.actions:\n",
        "          for state2 in self.states:\n",
        "              if sum(self.observed_transitions[state1][action1]) > 0:\n",
        "                self.P[state1][action1][state2] = self.observed_transitions[state1][action1][state2] / sum(self.observed_transitions[state1][action1])\n",
        "              else:\n",
        "                self.P[state1][action1][state2] = 0\n",
        "    \n",
        "    def print_policy(self):\n",
        "        for s in self.states:\n",
        "            a = self.policy[s]\n",
        "            print(self.original_states[s], self.original_actions[a])\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vnGR1zs2dhj",
        "outputId": "6003642b-a035-40d7-d399-1bcaf46dcdd1"
      },
      "source": [
        "# Usage - print initial policy\n",
        "\n",
        "opt_rl = OptimisticRL(states, actions, max([reward(s) for s in states]))\n",
        "curr_state = 'PU'\n",
        "opt_rl.value_iteration()\n",
        "print('initial policy: ')\n",
        "opt_rl.print_policy()\n",
        "\n",
        "# Usage - print learned policy\n",
        "\n",
        "n_iter = 100\n",
        "for j in range(n_iter):\n",
        "    opt_rl.value_iteration()\n",
        "    a = opt_rl.choose_action(curr_state)\n",
        "    next_state = get_next_state(curr_state, a)\n",
        "    opt_rl.learn(curr_state, a, reward(curr_state), next_state)\n",
        "    curr_state = next_state\n",
        "print('\\nlearned policy: ')\n",
        "opt_rl.print_policy()\n",
        "print('\\nestimated value: ')\n",
        "opt_rl.V"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial policy: \n",
            "PU save_money\n",
            "PF save_money\n",
            "RU save_money\n",
            "RF save_money\n",
            "\n",
            "\n",
            "learned policy: \n",
            "PU advertise\n",
            "PF save_money\n",
            "RU save_money\n",
            "RF save_money\n",
            "\n",
            "\n",
            "estimated value: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 29.15463929286255,\n",
              " 1: 35.30951072583212,\n",
              " 2: 44.06945022961627,\n",
              " 3: 52.059528109155664}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    }
  ]
}